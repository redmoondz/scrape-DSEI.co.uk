#!/usr/bin/env python3
"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π DSEI.co.uk Company Data Scraper

–≠—Ç–æ—Ç —Å–∫—Ä–∞–ø–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–æ 15 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á:
1. –ü–æ–ª—É—á–∞–µ—Ç –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–µ—Å—É—Ä—Å–∞
2. –°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –±–ª–æ–∫–∏ –∫–æ–º–ø–∞–Ω–∏–π (li —Ç–µ–≥–∏ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–ª–∞—Å—Å–æ–º)
3. –ò–∑–≤–ª–µ–∫–∞–µ—Ç slugs –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
4. –í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å—ã –∫ API –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ CSV: company_name, slug_name, url, stand, tags, overview, website
"""

import asyncio
import aiohttp
import aiofiles
import csv
import re
import logging
import signal
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin, quote
from typing import List, Dict, Optional, Set
import json
from pathlib import Path

from .config import Config


class AsyncDSEICompanyScraper:
    def __init__(self, config: Optional[Config] = None, max_concurrent_tasks: Optional[int] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ DSEI Company Scraper
        
        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
            max_concurrent_tasks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á. –ï—Å–ª–∏ None, –±–µ—Ä–µ—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        """
        self.config = config or Config()
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        if max_concurrent_tasks is None:
            async_config = self.config.get_async_config()
            max_concurrent_tasks = async_config.get('max_concurrent_tasks', 15)
        
        self.max_concurrent_tasks = max_concurrent_tasks
        
        # URLs –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.base_url = self.config.get_base_url()
        self.list_url_template = self.config.get_list_url_template()
        self.company_detail_url_template = self.config.get_company_detail_url_template()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –ø—É—Ç–µ–π
        self.project_root = Path(__file__).parent.parent.parent
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self._setup_logging()
        
        # –•—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        self.companies_data = []
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.delays = self.config.get_delays()
        self.timeouts = self.config.get_timeouts()
        self.selectors = self.config.get_selectors()
        self.max_retries = 3
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self.existing_companies: Set[str] = set()
        self.output_file_path = None
        self.processed_slugs: Set[str] = set()  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö slugs –≤–æ –≤—Ä–µ–º—è —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
        
        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        
        # –°–µ—Å—Å–∏—è –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –≤ async –º–µ—Ç–æ–¥–µ
        self.session: Optional[aiohttp.ClientSession] = None
        
        # –§–ª–∞–≥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        self.should_stop = False
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤
        self._setup_signal_handlers()
    
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / self.config.get_output_config().get('log_filename', 'scraper.log')
        
        # –û—á–∏—Å—Ç–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.getLogger().handlers.clear()
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _setup_signal_handlers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        def signal_handler(signum, frame):
            self.logger.info(f"üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º—É –∑–∞–≤–µ—Ä—à–µ–Ω–∏—é...")
            self.should_stop = True
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –¥–ª—è SIGINT (Ctrl+C) –∏ SIGTERM
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    async def _create_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π HTTP —Å–µ—Å—Å–∏–∏"""
        async_config = self.config.get_async_config()
        
        timeout = aiohttp.ClientTimeout(
            total=self.timeouts.get('request_timeout', 30),
            connect=async_config.get('connection_timeout', 10),
            sock_read=async_config.get('read_timeout', 30)
        )
        
        headers = {
            'User-Agent': self.config.get_user_agent(),
            'Accept': '*/*',
            'Accept-Language': 'ru,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br, zstd',
            'X-Requested-With': 'XMLHttpRequest',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Referer': self.base_url,
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-GPC': '1',
            'Priority': 'u=0'
        }
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        connector = aiohttp.TCPConnector(
            limit=async_config.get('connection_pool_size', 20),
            limit_per_host=async_config.get('per_host_limit', 10),
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers=headers,
            connector=connector
        )
    
    async def _close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π HTTP —Å–µ—Å—Å–∏–∏"""
        if self.session:
            await self.session.close()
    
    async def make_request_with_retry(self, url: str) -> Optional[str]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ HTTP –∑–∞–ø—Ä–æ—Å–∞ —Å –ª–æ–≥–∏–∫–æ–π –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∑–∞—â–∏—Ç—ã —Å–∞–π—Ç–∞
        
        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –æ—Ç–≤–µ—Ç–∞ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ None –µ—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å
        """
        if not self.session:
            await self._create_session()
            
        # –¢–∞–π–º–∞—É—Ç—ã –¥–ª—è –∑–∞—â–∏—Ç—ã —Å–∞–π—Ç–∞ (405, 429 –æ—à–∏–±–∫–∏)
        protection_timeouts = [30, 60, 90]  # —Å–µ–∫—É–Ω–¥—ã
        
        for attempt in range(self.max_retries):
            try:
                async with self.session.get(url) as response:
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞—â–∏—Ç—ã —Å–∞–π—Ç–∞
                    if response.status in [405, 429]:
                        timeout_index = min(attempt, len(protection_timeouts) - 1)
                        timeout_duration = protection_timeouts[timeout_index]
                        
                        self.logger.warning(
                            f"–°–∞–π—Ç –∑–∞—â–∏—â–∞–µ—Ç—Å—è (–∫–æ–¥ {response.status}) –¥–ª—è {url}. "
                            f"–û–∂–∏–¥–∞–Ω–∏–µ {timeout_duration} —Å–µ–∫—É–Ω–¥... (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_retries})"
                        )
                        
                        await asyncio.sleep(timeout_duration)
                        continue
                    
                    response.raise_for_status()
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
                    content = await response.text()
                    return content
                
            except aiohttp.ClientResponseError as e:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ HTTP –æ—à–∏–±–æ–∫
                if e.status in [405, 429]:
                    timeout_index = min(attempt, len(protection_timeouts) - 1)
                    timeout_duration = protection_timeouts[timeout_index]
                    
                    self.logger.warning(
                        f"HTTP –æ—à–∏–±–∫–∞ {e.status} –¥–ª—è {url}. "
                        f"–û–∂–∏–¥–∞–Ω–∏–µ {timeout_duration} —Å–µ–∫—É–Ω–¥... (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.max_retries})"
                    )
                    
                    await asyncio.sleep(timeout_duration)
                    continue
                else:
                    self.logger.warning(f"HTTP –æ—à–∏–±–∫–∞ {e.status} –¥–ª—è {url}: {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                self.logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                else:
                    self.logger.error(f"–í—Å–µ {self.max_retries} –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å –¥–ª—è {url}")
        
        return None
    
    async def load_existing_companies(self, file_path: Optional[Path] = None) -> int:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É CSV —Ñ–∞–π–ª—É. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π
        """
        if file_path is None:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—É—Ç–∏ –≤—ã–≤–æ–¥–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            data_dir = self.project_root / "data" / "processed"
            file_path = data_dir / self.config.get_output_config().get('csv_filename', 'dsei_companies.csv')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—É—Ç–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.output_file_path = file_path
        
        if not file_path.exists():
            self.logger.info(f"–°—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {file_path}. –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ.")
            return 0
        
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as csvfile:
                content = await csvfile.read()
                reader = csv.DictReader(content.splitlines())
                for row in reader:
                    company_name = row.get('company_name', '').strip()
                    if company_name:
                        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–º–µ–Ω–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –∫–∞–∫ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
                        self.existing_companies.add(company_name.lower())
            
            count = len(self.existing_companies)
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ {file_path}")
            return count
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ {file_path}: {e}")
            return 0
    
    def is_company_already_scraped(self, company_name: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª–∞ –ª–∏ –∫–æ–º–ø–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞
        
        Args:
            company_name: –ò–º—è –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ –∫–æ–º–ø–∞–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        return company_name.lower().strip() in self.existing_companies
    
    def add_company_to_existing(self, company_name: str):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –≤ –Ω–∞–±–æ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π
        
        Args:
            company_name: –ò–º—è –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        if company_name and company_name.strip():
            self.existing_companies.add(company_name.lower().strip())
    
    async def get_company_slugs_from_page(self, page_number: int) -> List[Dict[str, str]]:
        """
        –®–ê–ì 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞
        
        Args:
            page_number: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö slug –∫–æ–º–ø–∞–Ω–∏–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–µ–Ω–¥–µ
        """
        url = self.list_url_template.format(page=page_number)
        self.logger.info(f"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {url}")
        
        try:
            response = await self.make_request_with_retry(url)
            if not response:
                return []
            
            soup = BeautifulSoup(response, 'html.parser')
            
            # –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –±–ª–æ–∫–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
            companies_info = []
            # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∫–æ–º–ø–∞–Ω–∏–π
            company_containers = soup.find_all('li', class_='m-exhibitors-list__items__item')

            for container in company_containers:
                if isinstance(container, Tag):
                    # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ —ç—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
                    link = container.find('a', class_='js-librarylink-entry')
                    if link and isinstance(link, Tag):
                        href = link.get('href', '') or ''
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ slug –∏–∑ href —Ç–∏–ø–∞ "javascript:openRemoteModal('exhibitors-list/wind-river','ajax'..."
                        if isinstance(href, str):
                            match = re.search(r"'exhibitors-list/([^']+)'", href)
                            if match:
                                slug = match.group(1)
                                
                                # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å—Ç–µ–Ω–¥–µ –≤ —Ç–æ–º –∂–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                                stand = ""
                                stand_element = container.find('div', class_='m-exhibitors-list__items__item__header__meta__stand')
                                if stand_element:
                                    stand_text = stand_element.get_text(strip=True)
                                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞ —Å—Ç–µ–Ω–¥–∞ (—É–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞ "Stand: ")
                                    if stand_text.startswith('Stand:'):
                                        stand = stand_text.replace('Stand:', '').strip()
                                    else:
                                        stand = stand_text
                                
                                companies_info.append({
                                    'slug': slug,
                                    'stand': stand
                                })
                                self.logger.debug(f"–ù–∞–π–¥–µ–Ω slug –∫–æ–º–ø–∞–Ω–∏–∏: {slug}, —Å—Ç–µ–Ω–¥: {stand}")

            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞
            unique_companies = []
            seen_slugs = set()
            for company in companies_info:
                if company['slug'] not in seen_slugs:
                    unique_companies.append(company)
                    seen_slugs.add(company['slug'])

            if len(companies_info) != len(unique_companies):
                self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(companies_info) - len(unique_companies)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è slugs –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}")

            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(unique_companies)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}")
            return unique_companies
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}: {e}")
            return []
    
    async def get_company_details(self, company_slug: str, page_number: int, stand: str = "") -> Optional[Dict[str, str]]:
        """
        –®–ê–ì 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º slug –∫–æ–º–ø–∞–Ω–∏–∏
        
        Args:
            company_slug: URL slug –∫–æ–º–ø–∞–Ω–∏–∏
            page_number: –¢–µ–∫—É—â–∏–π –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è API –≤—ã–∑–æ–≤–∞
            stand: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç–µ–Ω–¥–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–ø–∏—Å–∫–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—è–º–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if self.should_stop:
            self.logger.debug(f"üõë –ü—Ä–æ–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {company_slug} - –ø–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            return None
            
        async with self.semaphore:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            url = self.company_detail_url_template.format(
                company_slug=quote(company_slug), 
                page=page_number
            )
            
            self.logger.debug(f"–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–∏: {url}")
            
            try:
                response = await self.make_request_with_retry(url)
                if not response:
                    return None
                
                soup = BeautifulSoup(response, 'html.parser')
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ –∫–æ–º–ø–∞–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                company_name = ""
                title_selector = self.selectors.get('company_title', 'h1.m-exhibitor-entry__item__header__title')
                title_element = soup.select_one(title_selector)
                if title_element:
                    company_name = title_element.get_text(strip=True)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–º–ø–∞–Ω–∏—è —É–∂–µ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –¥–∞
                if company_name and self.is_company_already_scraped(company_name):
                    self.logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ {company_name} - —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –≤—ã—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ")
                    return None
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤/–∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                tags = []
                category_selector = self.selectors.get('categories', 'li.m-exhibitor-entry__item__header__categories__item')
                category_elements = soup.select(category_selector)
                for cat_elem in category_elements:
                    tag = cat_elem.get_text(strip=True)
                    if tag:
                        tags.append(tag)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–∑–æ—Ä–∞/–æ–ø–∏—Å–∞–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                overview = ""
                desc_selector = self.selectors.get('description', 'div.m-exhibitor-entry__item__body__description')
                description_element = soup.select_one(desc_selector)
                if description_element:
                    overview = description_element.get_text(strip=True)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL —Å–∞–π—Ç–∞
                website = ""
                website_elements = soup.find_all('a', href=True)
                for link in website_elements:
                    if isinstance(link, Tag):
                        href = link.get('href', '') or ''
                        # –ü–æ–∏—Å–∫ –≤–Ω–µ—à–Ω–∏—Ö URL (–Ω–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫ —Å–∞–π—Ç–∞)
                        if isinstance(href, str) and href.startswith('http') and 'dsei.co.uk' not in href:
                            website = href
                            break
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ URL –∫–æ–º–ø–∞–Ω–∏–∏
                company_url = self.company_detail_url_template.format(
                    company_slug=quote(company_slug), 
                    page=page_number
                )
                
                company_data = {
                    'company_name': company_name,
                    'slug_name': company_slug,
                    'url': company_url,
                    'stand': stand,
                    'tags': '; '.join(tags),  # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π
                    'overview': overview.replace('\n', ' ').replace('\r', ' '),  # –û—á–∏—Å—Ç–∫–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
                    'website': website
                }
                
                self.logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {company_name}")
                return company_data
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –¥–ª—è {company_slug}: {e}")
                return None
    
    async def has_next_page(self, page_number: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Args:
            page_number: –¢–µ–∫—É—â–∏–π –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            True –µ—Å–ª–∏ –µ—Å—Ç—å –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        url = self.list_url_template.format(page=page_number + 1)
        
        try:
            response = await self.make_request_with_retry(url)
            if not response:
                return False
            
            soup = BeautifulSoup(response, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            company_links = soup.find_all('a', class_='js-librarylink-entry')
            
            return len(company_links) > 0
            
        except Exception:
            return False
    
    async def save_to_csv(self, filename: Optional[str] = None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–∞–π–ª
        
        Args:
            filename: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ CSV —Ñ–∞–π–ª–∞. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
        """
        if not self.companies_data:
            self.logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        if filename is None:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–∞—Ç–∞–ª–æ–≥ data/processed
            data_dir = self.project_root / "data" / "processed"
            data_dir.mkdir(parents=True, exist_ok=True)
            file_path = data_dir / self.config.get_output_config().get('csv_filename', 'dsei_companies.csv')
        else:
            file_path = Path(filename)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            file_exists = file_path.exists() and file_path.stat().st_size > 0
            
            # –û—Ç–∫—Ä—ã—Ç–∏–µ –≤ —Ä–µ–∂–∏–º–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Ä–µ–∂–∏–º–µ –∑–∞–ø–∏—Å–∏ –µ—Å–ª–∏ –Ω–æ–≤—ã–π
            mode = 'a' if file_exists else 'w'
            
            async with aiofiles.open(file_path, mode, encoding='utf-8', newline='') as csvfile:
                fieldnames = ['company_name', 'slug_name', 'url', 'stand', 'tags', 'overview', 'website']
                
                # –°–æ–∑–¥–∞–Ω–∏–µ CSV –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø–∞–º—è—Ç–∏
                import io
                output = io.StringIO()
                writer = csv.DictWriter(output, fieldnames=fieldnames)
                
                # –ó–∞–ø–∏—Å—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–æ–≤—ã–π/–ø—É—Å—Ç–æ–π
                if not file_exists:
                    writer.writeheader()
                
                for company in self.companies_data:
                    writer.writerow(company)
                
                # –ó–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
                await csvfile.write(output.getvalue())
            
            action = "–î–æ–±–∞–≤–ª–µ–Ω–æ" if file_exists else "–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ"
            self.logger.info(f"{action} {len(self.companies_data)} –∫–æ–º–ø–∞–Ω–∏–π –≤ {file_path}")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ CSV: {e}")
    
    async def auto_save_progress(self):
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        if self.companies_data:
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–æ–π
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            data_dir = self.project_root / "data" / "processed"
            data_dir.mkdir(parents=True, exist_ok=True)
            backup_filename = f"dsei_companies_backup_{timestamp}.csv"
            backup_path = data_dir / backup_filename
            
            try:
                await self.save_to_csv(str(backup_path))
                self.logger.info(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {backup_path}")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    async def process_companies_batch(self, companies_info: List[Dict[str, str]], page_number: int) -> List[Dict[str, str]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–π
        
        Args:
            companies_info: –°–ø–∏—Å–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö (slug –∏ stand)
            page_number: –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
        """
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö slugs
        companies_to_process = [
            company for company in companies_info 
            if company['slug'] not in self.processed_slugs
        ]
        
        if len(companies_to_process) < len(companies_info):
            skipped = len(companies_info) - len(companies_to_process)
            self.logger.info(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ {skipped} slugs - —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = []
        for company_info in companies_to_process:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if self.should_stop:
                self.logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤–æ –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–¥–∞—á...")
                break
                
            slug = company_info['slug']
            stand = company_info['stand']
            
            # –û—Ç–º–µ—Ç–∫–∞ slug –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ
            self.processed_slugs.add(slug)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ —Å –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            task = self.get_company_details(slug, page_number, stand)
            tasks.append(task)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∑–∞–¥–∞—á –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Å–µ–º–∞—Ñ–æ—Ä–∞
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful_companies = []
        for i, result in enumerate(results):
            if isinstance(result, dict):
                successful_companies.append(result)
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –Ω–∞–±–æ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                self.add_company_to_existing(result['company_name'])
                self.logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['company_name']}")
            elif isinstance(result, Exception):
                slug = companies_to_process[i]['slug']
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {slug}: {result}")
            else:
                # result is None - –º–æ–∂–µ—Ç –±—ã—Ç—å –¥—É–±–ª–∏–∫–∞—Ç
                slug = companies_to_process[i]['slug']
                self.logger.debug(f"‚è≠Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {slug} (–≤–æ–∑–º–æ–∂–Ω–æ, –¥—É–±–ª–∏–∫–∞—Ç)")
        
        return successful_companies
    
    async def scrape_all_companies(self, start_page: int = 1, max_pages: Optional[int] = None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–ª–µ–¥—É–µ—Ç –ª–æ–≥–∏–∫–µ –±–ª–æ–∫-—Å—Ö–µ–º—ã
        
        Args:
            start_page: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –Ω–∞—á–∞–ª–∞
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (None –¥–ª—è –≤—Å–µ—Ö)
        """
        self.logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ DSEI —Å–∫—Ä–∞–ø–µ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–π (–º–∞–∫—Å. {self.max_concurrent_tasks} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á)")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏
        await self._create_session()
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            await self.load_existing_companies()
            
            current_page = start_page
            pages_scraped = 0
            
            while True:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                if self.should_stop:
                    self.logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
                if max_pages and pages_scraped >= max_pages:
                    self.logger.info(f"üèÅ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü: {max_pages}")
                    break
                
                # –®–ê–ì 1: –ü–æ–ª—É—á–µ–Ω–∏–µ slugs –∫–æ–º–ø–∞–Ω–∏–π —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                companies_info = await self.get_company_slugs_from_page(current_page)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–º–ø–∞–Ω–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (—Ç–æ—á–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –≤ –±–ª–æ–∫-—Å—Ö–µ–º–µ)
                if not companies_info:
                    self.logger.info(f"üèÅ –ö–æ–º–ø–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page}. –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω.")
                    break
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                if self.should_stop:
                    self.logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–π...")
                    break
                
                # –®–ê–ì 2: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
                self.logger.info(f"‚ö° –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(companies_info)} –∫–æ–º–ø–∞–Ω–∏–π —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page}")
                
                batch_results = await self.process_companies_batch(companies_info, current_page)
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫ –æ–±—â–∏–º –¥–∞–Ω–Ω—ã–º
                self.companies_data.extend(batch_results)
                
                self.logger.info(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page}: {len(batch_results)} –Ω–æ–≤—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π")
                
                # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∫–∞–∂–¥—ã–µ 5 —Å—Ç—Ä–∞–Ω–∏—Ü)
                if pages_scraped % 5 == 0 and self.companies_data:
                    await self.auto_save_progress()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Ç–æ—á–∫–∞ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –≤ –±–ª–æ–∫-—Å—Ö–µ–º–µ)
                if not await self.has_next_page(current_page):
                    self.logger.info("üèÅ –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω.")
                    break
                
                # –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                current_page += 1
                pages_scraped += 1
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–µ—Ä–∂–∫–∏ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                if self.delays.get('between_pages', 0) > 0:
                    await asyncio.sleep(self.delays.get('between_pages', 2))
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            self.logger.info(f"üéâ –°–∫—Ä–∞–ø–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(self.companies_data)}")
            self.logger.info(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {pages_scraped + 1}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV
            await self.save_to_csv()
            
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è –°–∫—Ä–∞–ø–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C)")
            if self.companies_data:
                self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(self.companies_data)} –∫–æ–º–ø–∞–Ω–∏–π, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ —Å–∏—Ö –ø–æ—Ä...")
                await self.save_to_csv()
                await self.auto_save_progress()  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        
        except Exception as e:
            self.logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
            if self.companies_data:
                self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(self.companies_data)} –∫–æ–º–ø–∞–Ω–∏–π, —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–æ –æ—à–∏–±–∫–∏...")
                await self.save_to_csv()
                await self.auto_save_progress()  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        
        finally:
            # –ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏
            await self._close_session()


# –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞
async def run_async_scraper(config: Optional[Config] = None, 
                           start_page: int = 1, 
                           max_pages: Optional[int] = None,
                           max_concurrent_tasks: int = 15):
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–µ—Ä–∞
    
    Args:
        config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        start_page: –ù–∞—á–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
        max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
        max_concurrent_tasks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
    """
    scraper = AsyncDSEICompanyScraper(config, max_concurrent_tasks)
    await scraper.scrape_all_companies(start_page, max_pages)
